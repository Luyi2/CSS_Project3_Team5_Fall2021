{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Social Science Project #3 \n",
    "\n",
    "*Group number:* 5\n",
    "\n",
    "*Group members:* Nadia Almasalkhi, Daniel Lobo, Luyi Jian\n",
    "\n",
    "*Semester:* Fall 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Libraries setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "np.random.seed(273)\n",
    "\n",
    "# Make sure to import other libraries that will be necessary for training models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Pre-Processing and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspections Data 2011 - 2013\n",
    "chicago_inspections_2011_to_2013 = pd.read_csv(\"data/Chicago Inspections 2011-2013.csv\", \n",
    "                                              low_memory=False)\n",
    "\n",
    "# Inspections Data 2014\n",
    "chicago_inspections_2014 = pd.read_csv(\"data/Chicago Inspections 2014.csv\", \n",
    "                                      low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the inspections data\n",
    "chicago_inspections_2011_to_2013.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List column names\n",
    "chicago_inspections_2011_to_2013.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop column names related to geography, identification, and pass/fail flags that perfectly predict the outcome\n",
    "chicago_inspections_2011_to_2013.drop(columns = ['AKA_Name', \n",
    "                                                 'License',\n",
    "                                                'Address',\n",
    "                                                'City',\n",
    "                                                'State',\n",
    "                                                'Zip',\n",
    "                                                'Latitude',\n",
    "                                                'Longitude',\n",
    "                                                'Location',\n",
    "                                                'ID',\n",
    "                                                'LICENSE_ID',\n",
    "                                                 'LICENSE_TERM_START_DATE',\n",
    "                                                 'LICENSE_TERM_EXPIRATION_DATE',\n",
    "                                                 'LICENSE_STATUS',\n",
    "                                                'ACCOUNT_NUMBER',\n",
    "                                                'LEGAL_NAME',\n",
    "                                                'DOING_BUSINESS_AS_NAME',\n",
    "                                                'ADDRESS',\n",
    "                                                'CITY',\n",
    "                                                'STATE',\n",
    "                                                'ZIP_CODE',\n",
    "                                                'WARD',\n",
    "                                                'PRECINCT',\n",
    "                                                'LICENSE_CODE',\n",
    "                                                'BUSINESS_ACTIVITY_ID',\n",
    "                                                'BUSINESS_ACTIVITY',\n",
    "                                                'LICENSE_NUMBER',\n",
    "                                                'LATITUDE',\n",
    "                                                'LONGITUDE',\n",
    "                                                'pass_flag',\n",
    "                                                'fail_flag'],\n",
    "                                     inplace = True)\n",
    "\n",
    "chicago_inspections_2011_to_2013.set_index(['Inspection_ID', 'DBA_Name'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Inspection Date to a datetime format\n",
    "chicago_inspections_2011_to_2013['Inspection_Date'] = pd.to_datetime(chicago_inspections_2011_to_2013['Inspection_Date'], infer_datetime_format=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago_inspections_2011_to_2013.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do inspections look like over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Inspections Over Time\n",
    "chicago_inspections_2011_to_2013['Inspection_MonthYear'] = chicago_inspections_2011_to_2013['Inspection_Date'].dt.to_period('M')\n",
    "counts_by_day = chicago_inspections_2011_to_2013.groupby('Inspection_MonthYear').count().rename(columns = {'Facility_Type': 'Count'})['Count'].reset_index()\n",
    "counts_by_day.set_index([\"Inspection_MonthYear\"], inplace = True)\n",
    "counts_by_day.plot(title = \"Inspections by Month and Year\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do the results look like? \n",
    "\n",
    "**Answer:** It looks like the volume of inspections vary over the year, falling into lulls for months at a time (particularly around the holidays in December and in the month of July). It also appears that there were more inspections in 2013 than in 2012.\n",
    "<font color = 'blue'> The count of inspection in Jan. 2012 is over 500 and in Jan. 2013 is nearly 800, which seems to be not falling into lulls? -Luyi </font>\n",
    "\n",
    "<font color = 'green'> The line graph plot shows that the number of inspections varies pretty dramatically over the course of the year. Overall, we see fewer inspections in 2012 compared to the first three quarters of 2013. There also appears to be a trend of inspections rising in the winter months and declining in the spring/early summer months. -Daniel </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspection Results\n",
    "sns.catplot(data = chicago_inspections_2011_to_2013,\n",
    "           x = \"Results\",\n",
    "           kind = \"count\")\n",
    "\n",
    "plt.title(\"Inspection Results\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we separate by facility type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspection Results by Facility Type (Restaurant or Not)\n",
    "sns.catplot(data = chicago_inspections_2011_to_2013,\n",
    "           x = \"Results\",\n",
    "           kind = \"count\",\n",
    "           hue = 'Facility_Type_Clean')\n",
    "\n",
    "plt.title(\"Inspection Results by Facility Type\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop datetime info\n",
    "chicago_inspections_2011_to_2013 = chicago_inspections_2011_to_2013.dropna().drop(['Inspection_Date',\n",
    "                                      'minDate',\n",
    "                                      'maxDate',\n",
    "                                      'Inspection_MonthYear'],\n",
    "                                      axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set target variable. \n",
    "y = chicago_inspections_2011_to_2013['Results']\n",
    "## Comment out the following code if you don't want to binarize the target variable\n",
    "y = y.replace({'Pass w/ Conditions': 'Pass'})\n",
    "lb_style = LabelBinarizer()\n",
    "y = lb_style.fit_transform(y)\n",
    "# Recode 0s and 1s so 1s are \"Fail\"\n",
    "y = np.where(y == 1, 0 ,1)\n",
    "\n",
    "# All other features in X\n",
    "X = chicago_inspections_2011_to_2013.drop(columns = ['Results'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like we should merge `Facility_Type_1023`, because currently they appear as 4 slightly different columns:\n",
    "* `Facility_Type_1023 CHILDERN'S SERVICE FACILITY`\t\n",
    "* `Facility_Type_1023 CHILDERN'S SERVICE S FACILITY`\t\n",
    "* `Facility_Type_1023 CHILDERN'S SERVICES FACILITY`\t\n",
    "* `Facility_Type_1023-CHILDREN'S SERVICES FACILITY`\n",
    "* `Facility_Type_Children's Services Facility`\n",
    "\n",
    "-*Nadia*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To merge `Facility_Type_1023` - Luyi \n",
    "\n",
    "X['Facility_Type_1023 CHILDERNS SERVICE FACILITY'] = X.apply(lambda x: x[\"Facility_Type_1023 CHILDERN'S SERVICE FACILITY\"] + x[\"Facility_Type_1023 CHILDERN'S SERVICE S FACILITY\"] + x[\"Facility_Type_1023 CHILDERN'S SERVICES FACILITY\"] + x[\"Facility_Type_1023-CHILDREN'S SERVICES FACILITY\"] + x[\"Facility_Type_Children's Services Facility\"], axis=1)\n",
    "\n",
    "X = X.drop(columns = [\"Facility_Type_1023 CHILDERN'S SERVICE FACILITY\",\n",
    "                      \"Facility_Type_1023 CHILDERN'S SERVICE S FACILITY\",\n",
    "                     \"Facility_Type_1023 CHILDERN'S SERVICES FACILITY\",\n",
    "                     \"Facility_Type_1023-CHILDREN'S SERVICES FACILITY\",\n",
    "                     \"Facility_Type_Children's Services Facility\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"Facility_Type_1023 CHILDERNS SERVICE FACILITY\"] = X[\"Facility_Type_1023 CHILDERNS SERVICE FACILITY\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Fit Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training set and a validation set. Per the instructions, we do NOT create a test set.\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X, y, train_size = .80, test_size=0.20,\n",
    "                                                   stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Fit Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Model 1 - Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detail the basic logic and assumptions underlying each model, its pros/cons, and why it is a plausible choice for this problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM: The maximal margin classifier, the support vector classifier, and the support vector machine are often referred as “support vector machines”. Specifically, Maximal Margin Classifier (AKA Hard Margin)uses a hyperplane that separates the training observations perfectly according to their class labels; while Support Vector Classifier / Soft Margin Classifier use the same logic of hyperplane but allow misclassifying a few training observations to get better predictions in testing observations (namely, to avoid overfitting in training set). \n",
    "\n",
    "In other words, Hard Margin finds the hyperplane and margin that correctly classifies all points. It is sensitive to single observations/outliers, has the risk of overfitting, and will fail entirely if the data are not linearly separable. But Soft Margin, by allowing some points to “violate” the margin, trades off some bias for better variance (avoiding overfitting).\n",
    "\n",
    "For this problem, because there are so many features and we don't know if the data are linearly separable, it's better to use SVM to allow some fexibility in prediction. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(FROM THE INSTRUCTION) Be sure to do the following:\n",
    "\n",
    "1. Import the appropriate library from sklearn\n",
    "2. Set up a hyperparameter grid (check out our previous labs to see how to do this)\n",
    "3. Find the best hyperparameters, and then fit your model (using train/validation splits or cross-validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Untuned SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model\n",
    "svm = SVC()\n",
    "\n",
    "# fit the model\n",
    "svm_model = svm.fit(X_train, y_train.ravel())\n",
    "\n",
    "y_pred = svm_model.predict(X_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the confusion matrix of SVM \n",
    "cf_matrix = confusion_matrix(y_validate, y_pred, normalize = \"true\")\n",
    "\n",
    "df_cm = pd.DataFrame(cf_matrix, range(2),\n",
    "                  range(2))\n",
    "\n",
    "df_cm = df_cm.rename(index=str, columns={0: \"Pass\", 1: \"Fail\"})\n",
    "df_cm.index = [\"Pass\", \"Fail\"]\n",
    "plt.figure(figsize = (8,5))\n",
    "sns.set(font_scale=1.4)#for label size\n",
    "sns.heatmap(df_cm, \n",
    "           annot=True,\n",
    "           annot_kws={\"size\": 16},\n",
    "           fmt='g')\n",
    "\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation:\n",
    "From the confusion matrix, the SVM predicts well for facilities that passed the inspection (1.0), but predicts poorly for those failed the inspection (0.06), due to the imbalance of data structure where very few facilities failed inspection. Next we will search for optimal hyperparameters and see if the SVM with tuned parameters will perform better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### not able to get an output from this chunk of code, so I comment it out \n",
    "\n",
    "### use GridSearchCV to search for optimal hyperparameters  (Hyperparameter Tuning)\n",
    "\n",
    "#import warnings\n",
    "#from sklearn.exceptions import DataConversionWarning\n",
    "#warnings.filterwarnings(action='ignore')\n",
    "#from sklearn.metrics import accuracy_score\n",
    "\n",
    "### Define the hyperparameters to be tested out\n",
    "#svm_param_grid = {'C': [0.1, 1, 10],   # C: the regularization parameter, C, of the error term.\n",
    "              #'gamma': [1, 0.1, 0.01],  # gamma: the kernel coefficient for ‘rbf’, ‘poly’, and ‘sigmoid’. If gamma is ‘auto’, then 1/n_features will be used instead.\n",
    "              #'kernel': ['rbf']}  # kernel: specifies the kernel type to be used in the algorithm. It can be ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’, or a callable. The default value is ‘rbf’.\n",
    "\n",
    "### Initiate grid search for optimal hyperparameters\n",
    "#svm_grid = GridSearchCV(svm_model, svm_param_grid, cv=3)\n",
    "\n",
    "### Fit gird search on training data and save it as a grid of results\n",
    "#svm_grid_results = svm_grid.fit(X_train, y_train_rav)\n",
    "\n",
    "### Get the best model (the one that works best on left-out obs in cross validation)\n",
    "#best_model_svm = svm_grid_results.best_estimator_\n",
    "\n",
    "### Get the parameters that make up the best estimator\n",
    "#best_parameters = svm_grid_results.best_params_\n",
    "\n",
    "### Get the best results (Mean cross-validated score of the best_estimator)\n",
    "#best_results = svm_grid_results.best_score_\n",
    "\n",
    "### Print the best parameters and the best mean test score \n",
    "#print(\"Best parameters are\", best_parameters)\n",
    "#print(\"Best parameters produce mean test score of\", best_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get best predictions by predicting on validation set\n",
    "#best_svm_pred = best_model_svm.predict(X_validate)\n",
    "\n",
    "### create a confusion matrix to evaluate the tuned svm model on the validation set\n",
    "#cf_matrix_svm = confusion_matrix(y_validate_rav, best_svm_pred, normalize = \"true\")\n",
    "\n",
    "#df_cm = pd.DataFrame(cf_matrix_svm, range(2),\n",
    "                      #range(2))\n",
    "\n",
    "#df_cm = df_cm.rename(index=str, columns={0: \"Pass\", 1: \"Fail\"})\n",
    "#df_cm.index = [\"Pass\", \"Fail\"]\n",
    "#plt.figure(figsize = (8,5))\n",
    "#sns.set(font_scale=1.4)#for label size\n",
    "#sns.heatmap(df_cm, \n",
    "           #annot=True,\n",
    "           #annot_kws={\"size\": 16},\n",
    "           #fmt='g')\n",
    "\n",
    "#plt.title(\"Confusion Matrix for Tuned SVM Model on Validation Set\")\n",
    "#plt.xlabel(\"Predicted Label\")\n",
    "#plt.ylabel(\"True Label\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine Validation Metrics\n",
    "In this section, accuracy, precision, recall, and F1 score will be caculated based on the performance of untuned SVM model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the predictions from the untuned SVM model on the validation set.\n",
    "\n",
    "y_validate_pred_svm = svm_model.predict(X_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "A measure of the number of correction predictions regardless of direction, divided by the total number observations. Accuracy can be expressed as:\n",
    "\n",
    "$$\n",
    "Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = 0\n",
    "FP = 0\n",
    "TN = 0\n",
    "FN = 0\n",
    "\n",
    "for i in range(len(y_validate_pred_svm)): \n",
    "    if y_validate[i]==y_validate_pred_svm[i]==1:\n",
    "       TP += 1\n",
    "    if y_validate_pred_svm[i]==1 and y_validate[i]!=y_validate_pred_svm[i]:\n",
    "       FP += 1\n",
    "    if y_validate[i]==y_validate_pred_svm[i]==0:\n",
    "       TN += 1\n",
    "    if y_validate_pred_svm[i]==0 and y_validate_pred_svm[i]!=y_validate[i]:\n",
    "       FN += 1\n",
    "\n",
    "accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
    "print(\"Accuracy of the untuned SVM model is\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The untuned SVM model's accuracy (=0.853) is fair, but may not as good as the accuracy of Decision Tree classifier or Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "Precision is a measure of how well calibrated predictions are. Specifically, it tells us of the predictions in the positive class (\"failing inspection\" in this case) we made, how many were correct. The formula for precision is:\n",
    "\n",
    "$$\n",
    "Precision = \\frac{TP}{TP + FP}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = TP/(TP + FP)\n",
    "print(\"Precision of the untuned SVM mdoel is\", precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output: Precision of the untuned SVM mdoel is 0.9158415841584159"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall\n",
    "Recall is a measure that tells us, of all of the positive class members in the ground truth labels, how many did we successfully predict as positive? Recall is defined as:\n",
    "\n",
    "$$\n",
    "Recall = \\frac{TP}{TP + FN}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = TP/(TP+FN)\n",
    "print(\"Recall of the untuned SVM mdoel is\", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output: Recall of the untuned SVM mdoel is 0.2993527508090615\n",
    "\n",
    "This low value of recall is not surprising, given the untuned SVM model does a poor job predicting \"Failed\" cases (0.3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score\n",
    "The F1 Score is defined as:\n",
    "\n",
    "$$\n",
    "F1 = 2 * \\frac{precision * recall}{precision + recall}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = 2 * (precision * recall)/(precision + recall)\n",
    "print(\"F1 Score for the untuned SVM mdoel is\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output: F1 Score for the untuned SVM mdoel is 0.45121951219512196"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Model 2 - Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees are are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. A tree can be seen as a piecewise constant approximation. \n",
    "\n",
    "Some advatanges of Decision Trees are the following: They are simple to understand, to interpret, and to visualize. They require little data preparation and are able to handle both numerical and categorical data. They are considered white box models instead of black box models, meaning that if a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic. Black box models on the other hand, like neural networks, are more difficult to interpret. Decision trees are able to handle multi-output problems. It is possible to validate a Decision Tree model using statistical tests, making it possible to account for the reliability of the model. Lastly, Decision Trees perform well even if its assumptions are somewhat violated by the true model from which the data were generated.\n",
    "\n",
    "Some disadvantages of Decision Trees are the following: Overly complex Decision Trees can overfit to the data. Mechanisms such as pruning, setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem. Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble. Becuase Decision Tree predictions are piece-wise constant approximations and not continuous, they are not good at extrapolation. Practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.\n",
    "\n",
    "The Decision Tree Classifier is a plausible choice for this problem because our data is non-linear and our outcome variable is not continuous. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Decision Tree Classifier\n",
    "dt_classifier = tree.DecisionTreeClassifier(criterion='gini',  # or 'entropy' for information gain\n",
    "                       splitter='best',  # or 'random' for random best split\n",
    "                       max_depth=None,  # how deep tree nodes can go\n",
    "                       min_samples_split=2,  # samples needed to split node\n",
    "                       min_samples_leaf=1,  # samples needed for a leaf\n",
    "                       min_weight_fraction_leaf=0.0,  # weight of samples needed for a node\n",
    "                       max_features=None,  # number of features to look for when splitting\n",
    "                       max_leaf_nodes=None,  # max nodes\n",
    "                       min_impurity_decrease=1e-07, #early stopping\n",
    "                       random_state = 10) #random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cross validation to train the model on our data \n",
    "scores = cross_val_score(dt_classifier, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the mean score from the results of cross validation\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation: \n",
    "We see that our Decision Tree Classifier model is about 90 percent accurate in predicting inspection results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A unique aspect of tree-based methods is feature importance. One way to calculate feature importance is to see how much information each new feature adds. If a feature does not add any or very little information to a prediction, it may be possible to safely drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_classifier.fit(X_train, y_train)\n",
    "feat_importances = pd.concat([pd.DataFrame(X.columns),pd.DataFrame(np.transpose(dt_classifier.feature_importances_))], axis = 1)\n",
    "feat_importances.columns = [\"Feature\", \"Importance\"]\n",
    "sns.barplot(x = \"Importance\", y = \"Feature\", data = feat_importances.nlargest(10, 'Importance'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation: \n",
    "We see that seriousCount is by far the most important feature in the model, far more important than the the next 9 most important features combined. We may worry that this feature is biasing our results preductions given its outsized importance relative to the other most important features. Thus, we will try removing it from the model and see how our predictions are affected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list with the top 9 features for the Decision Tree Classifier, excluding 'seriousCount'\n",
    "dt_feature_df = feat_importances.nlargest(10, 'Importance')\n",
    "dt_feature_list = list(dt_feature_df[\"Feature\"])\n",
    "dt_feature_list.remove('seriousCount')\n",
    "dt_feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dataset X to include only the select 10 features\n",
    "reduced_X = X[dt_feature_list]\n",
    "reduced_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the reduced data into training and test set\n",
    "reduced_X_train, reduced_X_validate, y_train, y_validate = train_test_split(reduced_X, y, train_size = .80, test_size=0.20,\n",
    "                                                   stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-fit the Decision Tree Classifier on reduced data\n",
    "dt_classifier.fit(reduced_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cross validation to train the model on our reduced data \n",
    "scores = cross_val_score(dt_classifier, reduced_X, y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation: \n",
    "We see that dropping all but the 9 most important features of the model (exlcuding 'seriousCount') dramatically reduces the accuracy of our Decision Tree Classifier's predictions on the training set from 90 percent to 68 percent. Thus, we will keep our original full feature set for our DT model predictions moving forward.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing our Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To visualize our Decision Tree Classifier model on the original data set\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "_ = tree.plot_tree(dt_classifier, \n",
    "                   feature_names=X.columns,  \n",
    "                   class_names=[\"<=50k\", \">50k\"],\n",
    "                   filled=True,\n",
    "                  fontsize = 10,\n",
    "                  max_depth = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can visualize the model predictions on the validation set using a Confusion Matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dt_classifier.predict(X_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = confusion_matrix(y_validate, y_pred, normalize = \"true\")\n",
    "\n",
    "df_cm = pd.DataFrame(cf_matrix, range(2),\n",
    "                  range(2))\n",
    "\n",
    "df_cm = df_cm.rename(index=str, columns={0: \"Pass\", 1: \"Fail\"})\n",
    "df_cm.index = [\"Pass\", \"Fail\"]\n",
    "plt.figure(figsize = (8,5))\n",
    "sns.set(font_scale=1.4)#for label size\n",
    "sns.heatmap(df_cm, \n",
    "           annot=True,\n",
    "           annot_kws={\"size\": 16},\n",
    "           fmt='g')\n",
    "\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation: \n",
    "We see that our Decision Tree Classifier is a pretty strong model. It correctly predicts a passing inspection 94 percent of the time and correctly predicts a failing inspection 80 percent of the time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot(y, kde = False)\n",
    "ax.set_title(\"Distribution of Target Variable (Inspection Results)\")\n",
    "ax.set(xlabel='Variable Value', ylabel='Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation: \n",
    "By plotting our target variable, we can see that a class imbalance between passing inspections (coded as 0) and failing inspections (coded as 1) is likely driving the poorer predictions of failing inspections from our Decision Tree Classifier model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "To prevent overfitting of the Decision Tree Classifier model, one thing we could do is define some parameter which ends the recursive splitting process. This may be a parameter such as maximum tree depth or minimum number of samples required in a split. Controlling these model hyperparameters is the easiest way to counteract overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tools for evaluating Decision Tree Classifiers\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Define the hyperparameters to be tested out\n",
    "# Options include the default and an alternate option\n",
    "param_grid_dt = {\n",
    "    'criterion': ['gini','entropy'],\n",
    "    'max_depth': range(1,10),\n",
    "    'min_samples_split': range(1,10),\n",
    "    'min_samples_leaf': range(1,5),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate grid search\n",
    "dt_grid_search = GridSearchCV(dt_classifier, param_grid_dt, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit grid search to training data and save it as a grid of results\n",
    "dt_grid_results = dt_grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the best model (the one that works best on left-out obs in cross validation)\n",
    "best_model_dt = dt_grid_results.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicate the parameters that make up the best estimator\n",
    "best_parameters = dt_grid_results.best_params_\n",
    "\n",
    "# Identify the best results (Mean cross-validated score of the best_estimator)\n",
    "best_results = dt_grid_results.best_score_\n",
    "\n",
    "# Let's see what we're working with\n",
    "print(\"Best parameters are\", best_parameters)\n",
    "print(\"Best parameters produce mean test score of\", best_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation: \n",
    "Tuning the hyperparameters of our Decision Tree Classifier model reduces its accuracy on our training set from 90 percent to 80 percent. This could indicate less hyperfitting to the training data set, which may be a good thing. Let's see how this model compares on predictions on the validation data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix of Tuned Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model_dt.predict(X_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range(len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = confusion_matrix(y_validate, y_pred, normalize = \"true\")\n",
    "\n",
    "df_cm = pd.DataFrame(cf_matrix, range(2),\n",
    "                  range(2))\n",
    "\n",
    "df_cm = df_cm.rename(index=str, columns={0: \"Pass\", 1: \"Fail\"})\n",
    "df_cm.index = [\"Pass\", \"Fail\"]\n",
    "plt.figure(figsize = (8,5))\n",
    "sns.set(font_scale=1.4)#for label size\n",
    "sns.heatmap(df_cm, \n",
    "           annot=True,\n",
    "           annot_kws={\"size\": 16},\n",
    "           fmt='g')\n",
    "\n",
    "plt.title(\"Confusion Matrix of Tuned Model on Validation Set\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation: \n",
    "I'm not sure why this Confusion Matrix is coming out like this...I'm going to ask KQ tomorrow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Model 2 - Decision Tree Classifier Validation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "A measure of the number of correction predictions regardless of direction, divided by the total number observations. Accuracy can be expressed as:\n",
    "\n",
    "$$\n",
    "Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range(len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = 0\n",
    "FP = 0\n",
    "TN = 0\n",
    "FN = 0\n",
    "\n",
    "for i in range(len(y_pred)): \n",
    "    if y_validate[i]==y_pred[i]==1:\n",
    "       TP += 1\n",
    "    if y_pred[i]==1 and y_validate[i]!=y_pred[i]:\n",
    "       FP += 1\n",
    "    if y_validate[i]==y_pred[i]==0:\n",
    "       TN += 1\n",
    "    if y_pred[i]==0 and y_pred[i]!=y_validate[i]:\n",
    "       FN += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
    "print(\"Accuracy is\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "Precision is a measure of how well calibrated predictions are. Specifically, it tells us of the predictions in the positive class (\"failing inspection\" in this case) we made, how many were correct. The formula for precision is:\n",
    "\n",
    "$$\n",
    "Precision = \\frac{TP}{TP + FP}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = TP/(TP + FP)\n",
    "print(\"Precision is\", precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall\n",
    "Recall is a measure that tells us, of all of the positive class members in the ground truth labels, how many did we successfully predict as positive? Recall is defined as:\n",
    "\n",
    "$$\n",
    "Recall = \\frac{TP}{TP + FN}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = TP/(TP + FN)\n",
    "print(\"Recall is\", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score\n",
    "The F1 Score is defined as:\n",
    "\n",
    "$$\n",
    "F1 = 2 * \\frac{precision * recall}{precision + recall}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = 2 * (precision * recall)/(precision + recall)\n",
    "print(\"F1 Score is\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation: \n",
    "In our case of predicting whether a Chicago business passes or fails a food inspection, where a true positive (TP) is correctly precicting that a business fails inspection and a false positive (FP) is incorrectly predicting that a business fails inspection, FPs and false negatives (FN) are high stakes errors. We don't want to distrupt the operations of a business that passes inspection with a FP because that would be unfair to business owners and potentially detrimental to the community. We don't want to allow business that actually fails inspection to continue serving food to customers with a FN because that could be harmful to the health of the community. \n",
    "\n",
    "Thus, we may want to prioritize the precision measure of true negatives (TNs) in this case. This would tell us, of all the establishments that we predicted to pass inspection, what proportion did we correctly predict (TN/ FN + TN)? This is perhaps a more urgent measure to know than overall accuracy since it gives us confidence that our predictive model is causing minimal harm to people in the community. The precision measure of true positives (TPs), assuming it is high enough, would give us confidence that our predictive model is causing minimal harm to food-serving businesses in the community. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 - Model 3: Random Forest Classifier\n",
    "\n",
    "The random forests method chooses which feature to use at a split from a random subsample of features. In a random forest, multiple decision trees (often 100) are created and predictions are made based on each decision tree in the forest's vote on how to classify a given case.\n",
    "\n",
    "The random subset used in Random Forests allows it to sidestep the issue of Decision Trees being \"greedy\" or shortsighted. Random Forests also allow for an estimation of probabilities rather than just straightforward \"decisions\" about how to classify the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the shape of the y_train array\n",
    "y_train.shape\n",
    "y_validate.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`y_train` and `y_validate` are irregularly shaped and will thus need to use `.ravel()` in order to work well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_rav = y_train.ravel()\n",
    "y_validate_rav = y_validate.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Random Forest classifier, with hyperparameters that leave it relatively unrestricted.\n",
    "\n",
    "rf_classifier = RandomForestClassifier(criterion='gini',  # same as default\n",
    "                       max_depth=None,  # same as default\n",
    "                       min_samples_split=2,  # same as default\n",
    "                       min_samples_leaf=1,  # same as default\n",
    "                       min_weight_fraction_leaf=0.0,  # same as default\n",
    "                       max_features=None,  # number of features to look for when splitting\n",
    "                       max_leaf_nodes=None,  # same as default\n",
    "                       min_impurity_decrease=1e-07, #early stopping\n",
    "                       random_state = 10) #random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit this model to the training data\n",
    "\n",
    "rf_model = rf_classifier.fit(X_train, y_train_rav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's evaluate the accuracy of the rt_classifier on the training set\n",
    "\n",
    "y_train_pred_rf = rf_model.predict(X_train) # here we create the prediction of the rf_model for the training set data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's display the accuracy of the RF model on the training set\n",
    "rf_cf_matrix = confusion_matrix(y_train_rav, y_train_pred_rf, normalize = \"true\")\n",
    "\n",
    "rf_cm = pd.DataFrame(rf_cf_matrix, range(2),\n",
    "                  range(2))\n",
    "\n",
    "rf_cm = rf_cm.rename(index=str, columns={0: \"Pass\", 1: \"Fail\"})\n",
    "rf_cm.index = [\"Pass\", \"Fail\"]\n",
    "plt.figure(figsize = (8,5))\n",
    "sns.set(font_scale=1.4) #for label size\n",
    "sns.heatmap(rf_cm, \n",
    "           annot=True,\n",
    "           annot_kws={\"size\": 16},\n",
    "           fmt='g')\n",
    "\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forests Model has 99% accuracy on the training set, which makes sense because that's what the model was trained on and we allowed the trees within the forest to go down to leaves of *n* = 1. The real test now will be to see how it performs on the validation set. If it performs poorly on the validation set, then that is a sign of the model overfitting the training data, and we would need to go back and restrict the overfitting of the rf_classifier by limiting the maximum depth, raising the bar for the minimum impurity decrease, or other changes to the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate predictions for validation set using rf_model\n",
    "y_validate_pred_rf = rf_model.predict(X_validate)\n",
    "\n",
    "rf_cf_matrix = confusion_matrix(y_validate_rav, y_validate_pred_rf, normalize = \"true\")\n",
    "\n",
    "rf_cm = pd.DataFrame(rf_cf_matrix, range(2),\n",
    "                  range(2))\n",
    "\n",
    "rf_cm = rf_cm.rename(index=str, columns={0: \"Pass\", 1: \"Fail\"})\n",
    "rf_cm.index = [\"Pass\", \"Fail\"]\n",
    "plt.figure(figsize = (8,5))\n",
    "sns.set(font_scale=1.4) #for label size\n",
    "sns.heatmap(rf_cm, \n",
    "           annot=True,\n",
    "           annot_kws={\"size\": 16},\n",
    "           fmt='g')\n",
    "\n",
    "plt.title(\"Confusion Matrix for Baseline RF Classifier Model on Validation Set\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`rf_model` does alright in predicting on the validation set (Passes are accurately predicted 93% of the time; Fails are accurately predicted 89% of the time) but perhaps it could do better. Let us turn to **hyperparameter tuning** to see how changes there could improve the generalizability of the RF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning\n",
    "# This is what was originally run to determine the optimal hyperparameters for X_train, y_train_rav\n",
    "# But it takes a long time, so we are not going to run it again.\n",
    "# Instead, we just show the code that we originally used, and hard-code the tuned model below.\n",
    "\n",
    "# CODE WE ORIGINALLY USED\n",
    "\n",
    "## Import tools for evaluating random forests\n",
    "#from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "## Define the hyperparameters to be tested out\n",
    "## Options include the default and an alternate option.\n",
    "#param_grid_rf = {\n",
    "#    'n_estimators': [100, 150],\n",
    "#    'max_depth': [10, None],\n",
    "#    'max_features': [None, 'sqrt'],\n",
    "#    'min_impurity_decrease': [0.0000001, 0.0],\n",
    "#    'random_state': [10]\n",
    "#}\n",
    "\n",
    "## Initiate grid search\n",
    "#rf_grid_search = GridSearchCV(rf_classifier, param_grid_rf, cv=3)\n",
    "\n",
    "## Fit grid search to training data and save it as a grid of results\n",
    "#rf_grid_results = rf_grid_search.fit(X_train, y_train_rav)\n",
    "\n",
    "## Determine the best model (the one that works best on left-out obs in cross validation)\n",
    "#best_model_rf = rf_grid_results.best_estimator_\n",
    "\n",
    "## Explicate the parameters that make up the best estimator\n",
    "#best_parameters = rf_grid_results.best_params_\n",
    "\n",
    "## Identify the best results (Mean cross-validated score of the best_estimator)\n",
    "#best_results = rf_grid_results.best_score_\n",
    "\n",
    "## Let's see what we're working with\n",
    "#print(\"Best parameters are\", best_parameters)\n",
    "#print(\"Best parameters produce mean test score of\", best_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best hyperparameters are {'max_depth': 10, 'max_features': None, 'min_impurity_decrease': 1e-07, 'n_estimators': 150, 'random_state': 10}\n",
    "\n",
    "This estimator's mean test score = 0.9269269708905233"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE TUNED MODEL\n",
    "# Here, we hard-code the best_model_rf so that we don't have to re-run the time-intensive tuning code\n",
    "best_estimator_rf = RandomForestClassifier(n_estimators=150, \n",
    "                                           max_depth=10,\n",
    "                                           max_features=None,\n",
    "                                           min_impurity_decrease=1e-07,\n",
    "                                           random_state=10)\n",
    "\n",
    "best_model_rf = best_estimator_rf.fit(X_train, y_train_rav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the parameters tested out, the best estimator decreased accuracy on the training set.\n",
    "# Let's see where this error is coming from by using a confusion matrix.\n",
    "best_y_train_pred = best_model_rf.predict(X_train)\n",
    "\n",
    "# create a confusion matrix to allow us to evaluate the tuned classification model on the training set\n",
    "best_model_rf_train = confusion_matrix(y_train_rav, best_y_train_pred, normalize = \"true\")\n",
    "\n",
    "best_rf_train_cm = pd.DataFrame(best_model_rf_train, range(2), range(2))\n",
    "\n",
    "best_rf_train_cm = best_rf_train_cm.rename(index=str, columns={0: \"Pass\", 1: \"Fail\"})\n",
    "best_rf_train_cm.index = [\"Pass\", \"Fail\"]\n",
    "plt.figure(figsize = (8,5))\n",
    "sns.set(font_scale=1.4) #for label size\n",
    "sns.heatmap(best_rf_train_cm, \n",
    "           annot=True,\n",
    "           annot_kws={\"size\": 16},\n",
    "           fmt='g')\n",
    "\n",
    "plt.title(\"Confusion Matrix for Tuned Model on Training Set\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the tuned model `best_model_rf` performs slightly worse (True Pass = 0.95, True Fail = 0.98) than the baseline model `rf_model` (True Pass = 0.99, True Fail = 0.99) on the *training* set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, what we lost in training set accuracy, we may have gained in *validation set* accuracy. Let's evaluate it now on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the above was done using the training set. \n",
    "# Now let's use this best model created with the gridsearch to predict on the validation set\n",
    "best_y_val_pred = best_model_rf.predict(X_validate)\n",
    "\n",
    "# create a confusion matrix to allow us to evaluate the tuned classification model on the validation set\n",
    "best_model_rf_cf = confusion_matrix(y_validate_rav, best_y_val_pred, normalize = \"true\")\n",
    "\n",
    "best_rf_cm = pd.DataFrame(best_model_rf_cf, range(2), range(2))\n",
    "\n",
    "best_rf_cm = best_rf_cm.rename(index=str, columns={0: \"Pass\", 1: \"Fail\"})\n",
    "best_rf_cm.index = [\"Pass\", \"Fail\"]\n",
    "plt.figure(figsize = (8,5))\n",
    "sns.set(font_scale=1.4) #for label size\n",
    "sns.heatmap(best_rf_cm, \n",
    "           annot=True,\n",
    "           annot_kws={\"size\": 16},\n",
    "           fmt='g')\n",
    "\n",
    "plt.title(\"Confusion Matrix for Tuned Model on Validation Set\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tuned model performed better than the baseline model, with 91% of Fails accurately predicted as such, compared with 89% of Fails accurately before, with the baseline model. The proportion of Passes accurately predicted as Passes dropped only a miniscule amount, from 93.8% to 93.2%. `best_model_rf` is superior to `rf_model`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way that we might restrict overfitting to the training data and promote accuracy on the validation set would be to reduce the features considered. Let's take a look at what the top features are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look to what factors are most important.\n",
    "feat_importances = pd.concat([pd.DataFrame(X.columns),pd.DataFrame(np.transpose(rf_model.feature_importances_))], axis = 1)\n",
    "feat_importances.columns = [\"Feature\", \"Importance\"]\n",
    "sns.barplot(x = \"Importance\", y = \"Feature\", data = feat_importances.nlargest(20, 'Importance'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like only the top fifteen contribute much at all. The `seriousCount` seems to have a disproportionate effect on the model. Let's reduce the features included, removing seriousCount, and see if that helps us create more fine-grained results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_restricted = X_train.drop(columns='seriousCount')\n",
    "#X_validate_restricted = X_validate.drop(columns='seriousCount')\n",
    "\n",
    "X_noSC = X.drop(columns=\"seriousCount\")\n",
    "# split the reduced data into training and validation set\n",
    "X_train_restricted, X_validate_restricted, y_train_restricted, y_validate_restricted = train_test_split(X_noSC, y, train_size=0.80, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check if y_train_restricted and y_validate_restricted are in the right shape or need to be raveled.\n",
    "print(y_train_restricted.shape, y_validate_restricted.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's fix the shape of the y sets\n",
    "y_train_restricted = y_train_restricted.ravel()\n",
    "y_validate_restricted = y_validate_restricted.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we're fitting on a different set of features, we need to re-do the hyperparameter tuning\n",
    "\n",
    "# Below is the code we WOULD run if we had the computer power\n",
    "\n",
    "## Define the hyperparameters to be tested out\n",
    "## Options include the default and an alternate option\n",
    "#param_grid_rf_restricted = {\n",
    "#    'n_estimators': [100, 150],\n",
    "#    'max_depth': [10, None],\n",
    "#    'max_features': [None, 'sqrt'],\n",
    "#    'max_leaf_nodes': [100, 200],\n",
    "#    'min_impurity_decrease': [0.0000001, 0.0],\n",
    "#    'random_state': [10]\n",
    "#}\n",
    "\n",
    "## Initiate grid search\n",
    "#rf_grid_search_restricted = GridSearchCV(rf_classifier, param_grid_rf_restricted, cv=3)\n",
    "\n",
    "## Fit grid search to training data and save it as a grid of results\n",
    "#rf_grid_results_restricted = rf_grid_search_restricted.fit(X_train_restricted, y_train_restricted)\n",
    "\n",
    "## Determine the best model (the one that works best on left-out obs in cross validation)\n",
    "#best_model_rf_restricted = rf_grid_results_restricted.best_estimator_\n",
    "\n",
    "## Explicate the parameters that make up the best estimator\n",
    "#best_parameters_restricted = rf_grid_results_restricted.best_params_\n",
    "\n",
    "## Identify the best results (Mean cross-validated score of the best_estimator)\n",
    "#best_results_restricted = rf_grid_results_restricted.best_score_\n",
    "\n",
    "## Let's see what we're working with\n",
    "#print(\"Best parameters are\", best_parameters_restricted)\n",
    "#print(\"Best parameters produce mean test score of\", best_results_restricted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# However, we don't have the necessary computer power or time, so instead we'll fit a model using the hyperparameters\n",
    "# selected by the previous grid search, AKA best_model_rf\n",
    "\n",
    "best_model_rf_restricted = best_estimator_rf.fit(X_train_restricted, y_train_restricted)\n",
    "\n",
    "# Then, we make a prediction using this model and evaluate how it does on the validation set.\n",
    "\n",
    "# Predictions using the \"tuned\" model fitted to the data excluding \"seriousCount\"\n",
    "best_y_val_pred_restricted = best_model_rf_restricted.predict(X_validate_restricted)\n",
    "\n",
    "# create a confusion matrix to allow us to evaluate the tuned classification model on the validation set\n",
    "best_model_rf_cf_restricted = confusion_matrix(y_validate_restricted, best_y_val_pred_restricted, normalize = \"true\")\n",
    "\n",
    "best_rf_cm_restricted = pd.DataFrame(best_model_rf_cf_restricted, range(2), range(2))\n",
    "\n",
    "best_rf_cm_restricted = best_rf_cm_restricted.rename(index=str, columns={0: \"Pass\", 1: \"Fail\"})\n",
    "best_rf_cm_restricted.index = [\"Pass\", \"Fail\"]\n",
    "plt.figure(figsize = (8,5))\n",
    "sns.set(font_scale=1.4) #for label size\n",
    "sns.heatmap(best_rf_cm_restricted, \n",
    "           annot=True,\n",
    "           annot_kws={\"size\": 16},\n",
    "           fmt='g')\n",
    "\n",
    "plt.title(\"Confusion Matrix for Restricted Tuned Model on Validation Set\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** The best Random Forests model to use is `best_model_rf` which is fit to all the features in `X_train` and `y_train.ravel()` (AKA `y_train_rav`) and uses the following hyperparameters:\n",
    "\n",
    "* {'max_depth': 10, 'max_features': None, 'min_impurity_decrease': 1e-07, 'n_estimators': 150, 'random_state': 10}\n",
    "\n",
    "All hyperparameters not mentioned use the default hyperparameters provided by `sklearn.ensemble.RandomForestClassifier`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3  - Random Forests Classifier Validation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, accuracy, precision, recall, F1 score, and cross_val_score will be determined for the Random Forests classifier on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_validate_rav.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the predictions for the validation set.\n",
    "\n",
    "y_validate_pred = best_model_rf.predict(X_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"purple\"> I don't know why this problem is happening. Halp.\n",
    "    -Nadia </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "A measure of the number of correction predictions regardless of direction, divided by the total number observations. Accuracy can be expressed as:\n",
    "\n",
    "$$\n",
    "Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = 0\n",
    "FP = 0\n",
    "TN = 0\n",
    "FN = 0\n",
    "\n",
    "for i in range(len(y_validate_pred)): \n",
    "    if y_validate[i]==y_validate_pred[i]==1:\n",
    "       TP += 1\n",
    "    if y_validate_pred[i]==1 and y_validate[i]!=y_validate_pred[i]:\n",
    "       FP += 1\n",
    "    if y_validate[i]==y_validate_pred[i]==0:\n",
    "       TN += 1\n",
    "    if y_validate_pred[i]==0 and y_validate_pred[i]!=y_validate[i]:\n",
    "       FN += 1\n",
    "\n",
    "accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
    "print(\"Accuracy of the Random Forests model is\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest's accuracy (=0.927) is a slight improvement over the Decision Tree classifier's accuracy (=0.914)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "Precision is a measure of how well calibrated predictions are. Specifically, it tells us of the predictions in the positive class (\"failing inspection\" in this case) we made, how many were correct. The formula for precision is:\n",
    "\n",
    "$$\n",
    "Precision = \\frac{TP}{TP + FP}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = TP/(TP + FP)\n",
    "print(\"Precision of the Random Forests mdoel is\", precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decision Tree classifier's precision (=0.781) is slightly better than Random Forests precision (=0.771)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall\n",
    "Recall is a measure that tells us, of all of the positive class members in the ground truth labels, how many did we successfully predict as positive? Recall is defined as:\n",
    "\n",
    "$$\n",
    "Recall = \\frac{TP}{TP + FN}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = TP/(TP+FN)\n",
    "print(\"Recall of the Random Forests mdoel is\", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forests model performs *significantly* better (recall=0.910) than the Decision Tree model (recall=0.795) in this measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score\n",
    "The F1 Score is defined as:\n",
    "\n",
    "$$\n",
    "F1 = 2 * \\frac{precision * recall}{precision + recall}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = 2 * (precision * recall)/(precision + recall)\n",
    "print(\"F1 Score for Random Forests is\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forests f1 score (=0.835) is moderately better than that of the Decision Tree model (=0.788)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(rf_model, X_validate, y_validate, cv=5)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(rf_model, X, y.ravel(), cv=5)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean cross validation scores for the Random Forests model, both when run on the validation set alone (mean cross val score = 0.919) and on the full set (mean cross val score = 0.922), are higher than the full-set mean cross_val_score of the Decision Tree model (=0.905) and the reduced Decision Tree model (=0.892)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.  Policy Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1  Interpretable Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint**: Use tools like feature importance plots and coefficient plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Model 1 - SVM Feature Importance Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, svm\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_indices = np.arange(X.shape[-1])\n",
    "# Univariate feature selection with F-test for feature scoring\n",
    "# We use the default selection function: the 10% most significant features\n",
    "selector = SelectPercentile(f_classif, percentile=10)\n",
    "selector.fit(X, y.ravel())\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "scores /= scores.max()\n",
    "plt.bar(X_indices - .45, scores, width=.2,\n",
    "        label=r'Univariate score ($-Log(p_{value})$)', color='g')\n",
    "\n",
    "# Compare to the weights of an SVM\n",
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(X, y.ravel())\n",
    "\n",
    "svm_weights = (clf.coef_ ** 2).sum(axis=0)\n",
    "svm_weights /= svm_weights.max()\n",
    "\n",
    "plt.bar(X_indices - .25, svm_weights, width=.2, label='SVM weight', color='r')\n",
    "\n",
    "clf_selected = svm.SVC(kernel='linear')\n",
    "clf_selected.fit(selector.transform(X), y.ravel())\n",
    "\n",
    "svm_weights_selected = (clf_selected.coef_ ** 2).sum(axis=0)\n",
    "svm_weights_selected /= svm_weights_selected.max()\n",
    "\n",
    "plt.bar(X_indices[selector.get_support()] - .05, svm_weights_selected,\n",
    "        width=.2, label='SVM weights after selection', color='b')\n",
    "\n",
    "plt.title(\"Comparing feature selection\")\n",
    "plt.xlabel('Feature number')\n",
    "plt.yticks(())\n",
    "plt.axis('tight')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# got an error that SVC' object has no attribute 'feature_importances_' if using code from the lab\n",
    "# the above code is from https://www.cnpython.com/qa/321322\n",
    "# but not able to run through "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a dataframe with coefficients and feature names\n",
    "svm_data = pd.DataFrame([svm_model.coef_, \n",
    "                             chicago_inspections_2011_to_2013.columns]).T\n",
    "\n",
    "## Output gets error message: \"coef_ is only available when using a linear kernel\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Model 2 - Decision Tree Classifier Feature Importance Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replicating Feature Importance Plot from 3.2.2 above\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "feat_importances = pd.concat([pd.DataFrame(X.columns),pd.DataFrame(np.transpose(dt_classifier.feature_importances_))], axis = 1)\n",
    "feat_importances.columns = [\"Feature\", \"Importance\"]\n",
    "sns.barplot(x = \"Importance\", y = \"Feature\", data = feat_importances.nlargest(10, 'Importance'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation: \n",
    "We see that seriousCount is by far the most important feature in the model, far more important than the the next 9 most important features combined. One feature that I wish we could incorporate into the model is which inspector from the Chicago Department of Public Health’s Food Protection Program completed past inpections. Although the inspections are done using a standardized procedure, I wonder if there are were inspectors that were systematically harsher or more lenient and how that might bias the results and subsequent predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2  Prioritize Audits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint**: Look up the [`.predict()`](https://www.kite.com/python/docs/sklearn.linear_model.SGDRegressor.predict), [`.predict_proba()`](https://www.kite.com/python/docs/sklearn.linear_model.LogisticRegression.predict_proba), and [`.sample()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sample.html) methods. Then: \n",
    "1. Choose one of your models (or train a new simplified model or ensemble!) to predict outcomes and probabilities. \n",
    "2. Order your audits by their probability of detecting a \"Fail\" score\n",
    "3. Plot your distribution of pass/fail among the first 1000 observations in the dataset\n",
    "4. Simulate random audits on the full chicago_2011_to_2013.csv dataset by picking 1000 observations at random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use Random Forests to determine which facilities to audit.\n",
    "\n",
    "Random Forests can use `predict_proba()`. In the parentheses we put the X array and then it returns an array where the second column is the proportion of decision trees within the random forest that classify that observation as 1 (signifying \"Fail\"). From there, we can select out the top 1000 observations with the highest probabilities (i.e., ratios of decision trees classifying them as 1) using `nlargest(1000)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set it up using the Random Forest model of our choice\n",
    "# Ensure that the model/estimator variable preceding .predict_proba() is the best model for the job\n",
    "prob_pred_rf_val = best_model_rf.predict_proba(X_validate)\n",
    "\n",
    "# prob_pred_rf_val is an array where the second column = predicted probabilities of Failed inspection\n",
    "\n",
    "# Let's print the array in descending order, sorted by the second number in each sub-list\n",
    "print(prob_pred_rf_val.sort(key=sortSecond,reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = \"purple\"> I am following an example I saw so I don't know what I'm doing wrong. Suggestions appreciated.\n",
    "    -Nadia </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to figure out how to select the 1000 riskiest establishments\n",
    "X['Risk_Risk 1 (High)'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago_inspections_2011_to_2013['seriousCount'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could try to taking a random sample of the establishments with the highest risk of 1 who also have a serious count >1? \n",
    "chicago_inspections_2011_to_2013['seriousCount'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[('Risk_Risk 1 (High)'== 1) & ('seriousCount'>1)].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do we need to change one of the dtypes to be able to run the above code?\n",
    "X.dtypes['Risk_Risk 1 (High)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dtypes['seriousCount']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3  Predict on Data with Unseen Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the code below with the X data you used for training\n",
    "X_test = chicago_inspections_2014[chicago_inspections_2014.columns & ....columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Discussion Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Why do we need metrics beyond accuracy when using machine learning in the social sciences and public policy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need metrics beyond accuracy when using machine learning in the social sciences and public policy because often times the direction of our prediction, that is, positive or negative class in this case, often has real-world implications that may cause people and communities harm. In our case, predicting a passing inspection (TN or FN) is particularly consequential. To the extent that we incorrectly predict passing inspections for food establishments, we may be putting members of the community in harm's way by enabling businesses that should not be operating to serve them unhealthy food. Thus, in addition to the overall accuracy of our predictive model, we would be particularly interested in the precision of our positive and negative classifications. This would likely be the case for many other problems in the social sciences and public policy.\n",
    "\n",
    "Anything to add?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Imagine that establishments learned about the algorithm being used to determine who gets audited and they started adjusting their behavior (and changing certain key features about themselves that were important for the prediction) to avoid detection. How could policymakers address this interplay between algorithmic decisionmaking and real world behavior?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm....I may ask KQ for some guidance on this question."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
